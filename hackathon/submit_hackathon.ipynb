{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ Welcome to the **Smart Droplets Hackathon Submission**!\n",
        "\n",
        "This notebook is here to help you test and submit your solution for the Smart Droplets Hackathon.  \n",
        "You can use it to validate and evaluate either:\n",
        "\n",
        "* A trained Reinforcement Learning (RL) agent, or  \n",
        "* A Conditional agent\n",
        "\n",
        "Please make sure to follow the instructions in each section carefully.\n",
        "\n",
        "Good luck â€” and have fun!\n",
        "\n",
        "> **Note:** The evaluation environment used in this notebook is the same \"validation\" environment provided in the hackathon notebook. For the final evaluation, your agent will be tested on an unseen test environment. This test environment is structurally identical to the validation environment, but uses weather data from a different location.\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/WUR-AI/A-scab/blob/hackathon/hackathon/submit_hackathon.ipynb)"
      ],
      "metadata": {
        "id": "yJP0G6QhD4nQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setup\n",
        "Before testing your agent, please make sure the environment is properly set up. Use the cell below to import dependencies and initialize the simulation environment."
      ],
      "metadata": {
        "id": "0OqBVIvzGt_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to set up the environment\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Clone the repository\n",
        "!rm -rf A-scab/\n",
        "!git clone -b hackathon https://github.com/WUR-AI/A-scab.git\n",
        "\n",
        "# Change directory\n",
        "%cd A-scab\n",
        "\n",
        "# Get the absolute path of the project root *after* changing directory\n",
        "PROJECT_ROOT = os.getcwd()\n",
        "\n",
        "# Install poetry if needed.\n",
        "!pip install -qqq poetry\n",
        "\n",
        "# Configure poetry to create virtual environments in the project directory\n",
        "!poetry config virtualenvs.in-project true\n",
        "\n",
        "# Install project dependencies\n",
        "# This step may take some time (e.g., 5 minutes)\n",
        "!poetry install --quiet --all-extras\n",
        "\n",
        "# These are things you don't need to know about for now :)\n",
        "\n",
        "# 1. Add the project root to sys.path\n",
        "if PROJECT_ROOT not in sys.path:\n",
        "    sys.path.insert(0, PROJECT_ROOT)\n",
        "    print(f\"Added project root {PROJECT_ROOT} to sys.path\")\n",
        "\n",
        "# 2. Get the path to the site-packages directory of the poetry virtual environment\n",
        "try:\n",
        "    venv_path_output = subprocess.check_output(['poetry', 'env', 'info', '--path']).decode('utf-8').strip()\n",
        "    python_version_major_minor = f\"python{sys.version_info.major}.{sys.version_info.minor}\"\n",
        "    SITE_PACKAGES_PATH = os.path.join(venv_path_output, 'lib', python_version_major_minor, 'site-packages')\n",
        "except Exception as e:\n",
        "    print(f\"Could not determine poetry venv site-packages path using 'poetry env info': {e}\")\n",
        "    print(\"Attempting a common fallback path structure...\")\n",
        "    SITE_PACKAGES_PATH = os.path.abspath(os.path.join('.venv', 'lib', f'python{sys.version_info.major}.{sys.version_info.minor}', 'site-packages'))\n",
        "\n",
        "# 3. Add the site-packages directory to sys.path\n",
        "if os.path.exists(SITE_PACKAGES_PATH) and SITE_PACKAGES_PATH not in sys.path:\n",
        "    sys.path.insert(0, SITE_PACKAGES_PATH)\n",
        "    print(f\"Added {SITE_PACKAGES_PATH} to sys.path\")\n",
        "else:\n",
        "    print(f\"Warning: Could not find site-packages at {SITE_PACKAGES_PATH} or it's already in sys.path.\")\n",
        "\n",
        "import ascab\n",
        "import gymnasium as gym"
      ],
      "metadata": {
        "id": "k2iwv6MgD58j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RL Agent\n",
        "If you have trained a Reinforcement Learning (RL) agent, please use the cells below to test your agent in the simulation environment. This will help validate that your agent behaves as expected and meets the requirements for submission."
      ],
      "metadata": {
        "id": "Cw6h63GjFthO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload Your Trained Agent\n",
        "Please run the cell below and use the resulting upload button to select and upload your files.\n",
        "\n",
        "Do not manually upload files; the code below ensures files are correctly processed and renamed automatically.\n",
        "\n",
        "Typically, you need to upload:\n",
        "\n",
        "1.   Your trained model file (e.g., `rl_agent.zip`)\n",
        "2.   (Optional) Environment normalization file, if you used one during training (e.g., `rl_agent_norm.pkl`)"
      ],
      "metadata": {
        "id": "TKqRhUf9JMZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Upload files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Create /content if it doesn't exist (mostly for local notebook support)\n",
        "os.makedirs('/content', exist_ok=True)\n",
        "\n",
        "# Standard filenames\n",
        "standard_model_name = \"rl_agent.zip\"\n",
        "standard_norm_name = \"rl_agent_norm.pkl\"\n",
        "\n",
        "def safe_move(src, dst):\n",
        "    if os.path.exists(dst):\n",
        "        os.remove(dst)\n",
        "    shutil.move(src, dst)\n",
        "\n",
        "# Move and rename uploaded files, allowing overwriting\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith(\".zip\") and \"norm\" not in filename.lower():\n",
        "        safe_move(filename, f\"/content/{standard_model_name}\")\n",
        "        print(f\"Renamed and moved '{filename}' â†’ '/content/{standard_model_name}'\")\n",
        "    elif filename.endswith(\".pkl\") or filename.endswith(\".pickle\") or \"norm\" in filename.lower():\n",
        "        safe_move(filename, f\"/content/{standard_norm_name}\")\n",
        "        print(f\"Renamed and moved '{filename}' â†’ '/content/{standard_norm_name}'\")"
      ],
      "metadata": {
        "id": "7CvTlI8EKjmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run you RL agent\n",
        "\n",
        "Now weâ€™ll load your trained RL agent and run it in the test environment.\n",
        "\n",
        "\n",
        "*   By default, it assumes you have used the DQN algorithm. If you trained your\n",
        "*   Similarly, if you customized the observation space, make sure to update the observation_filter to match your modelâ€™s expected inputs"
      ],
      "metadata": {
        "id": "qnbS0RETOKpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ascab.train import RLAgent\n",
        "from ascab.utils.plot import plot_results\n",
        "from stable_baselines3 import PPO, SAC, TD3, DQN, HER\n",
        "\n",
        "ascab_val = gym.make('AscabValEnv-Discrete')\n",
        "\n",
        "algo=DQN\n",
        "observation_filter=list(ascab_val.observation_space.keys())\n",
        "\n",
        "rl_agent = RLAgent(\n",
        "    rl_algorithm=algo,\n",
        "    observation_filter=list(ascab_val.observation_space.keys()),\n",
        "    ascab_test=ascab_val,\n",
        "    path_model='/content/rl_agent',\n",
        ")\n",
        "results = rl_agent.run()"
      ],
      "metadata": {
        "id": "ie87FqGbib0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare files for submission\n",
        "\n",
        "Congratulations! If the final cell executed without errors and you see rewards being reported, your reinforcement learning agent is functioning correctly in the evaluation environment.\n",
        "\n",
        "You're now ready to submit your results. Please follow these steps:\n",
        "\n",
        "1. Download your trained agent, and if applicable, the normalization file:  \n",
        "   - `/content/rl_agent.zip`  \n",
        "   - `/content/rl_agent_norm.pkl`\n",
        "\n",
        "2. Download this notebook to include it in your submission.\n",
        "\n",
        "3. Create a ZIP file containing the downloaded notebook and the trained agent files.\n",
        "\n",
        "4. Go to the submission section at the end of this notebook\n"
      ],
      "metadata": {
        "id": "013vjgEDHcBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conditional Agent\n",
        "\n",
        "If you have designed a conditional agent, please follow the instructions and code cells below to test and evaluate your model.\n",
        "\n",
        "The example code provided here defines a simple conditional agent. Replace this example with your own implementation, making sure it runs correctly in the evaluation environment."
      ],
      "metadata": {
        "id": "mOp1t5lHQ_4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conditional agent\n",
        "\n",
        "from ascab.train import BaseAgent\n",
        "from ascab.env.env import AScabEnv\n",
        "\n",
        "# First define the class, not forgetting to SubClass `BaseAgent`\n",
        "# In general, you want to change the `get_action` method to apply your conditional spraying strategy\n",
        "class HowMyLocalFarmerSprays(BaseAgent):\n",
        "    def __init__(\n",
        "        self,\n",
        "        ascab: AScabEnv = None,\n",
        "        render: bool = True,\n",
        "    ):\n",
        "        super().__init__(ascab=ascab, render=render)\n",
        "\n",
        "    def get_action(self, observation: dict = None) -> float:\n",
        "        # Please include this check for all your conditional agents to constrain it within the \"risk period\".\n",
        "        if self.ascab.get_wrapper_attr(\"info\")[\"InfectionWindow\"] and self.ascab.get_wrapper_attr(\"info\")[\"InfectionWindow\"][-1] == 1:\n",
        "            # The code below means: \"If it is forecasted that it will rain in two days, I will spray today\".\n",
        "            if self.ascab.get_wrapper_attr(\"info\")[\"Forecast_day2_HasRain\"] and self.ascab.get_wrapper_attr(\"info\")[\"Forecast_day2_HasRain\"][-1]:\n",
        "                return 1.0  # the agent sprays this much if is forecasted to rain in two days\n",
        "        return 0.0\n",
        "\n",
        "farmer_strategy = HowMyLocalFarmerSprays(ascab_val)\n",
        "\n",
        "# Use the class method below to run your agent!\n",
        "results = farmer_strategy.run()"
      ],
      "metadata": {
        "id": "1WLoBrFnz6fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare files for submission...\n",
        "\n",
        "Congratulations! If the final cell executed without errors and you see rewards being reported, your conditional agent is functioning correctly in the evaluation environment.\n",
        "\n",
        "You're now ready to submit your results. Please follow these steps:\n",
        "\n",
        "1. Download this notebook to include it in your submission.\n",
        "\n",
        "2. Zip the notebook that you just downloaded\n",
        "\n",
        "3. Go to the submission section at the end of this notebook"
      ],
      "metadata": {
        "id": "11e8DmiWI690"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submit your files"
      ],
      "metadata": {
        "id": "oHGJZcYS3MFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions\n",
        "\n",
        "- Go to this link in huggingface [LINK](https://huggingface.co/spaces/com3dian/hackathon), and there you can upload your submission of either a trained RL agent or a conditional trained agent.\n",
        "- In that huggingface link, the leaderboard is also displayed!\n",
        "- You should upload a `.zip` file with these specifications:\n",
        "    1. The file name should be in the format of `[NAME]_[MODEL_NAME].zip`. So, for example, Hilmy's submission would be `HILMY_AwesomeModel.zip`. It is possible to also do anonymous submissions, if you don't want your name on the leaderboard, which we perfectly understand if that's the case ðŸ™‚ In that case, you should also append another underscore, for example `HILMY_AwesomeModel_H4ck3r.zip`. This way will indicate that your nickname will be displayed in the leaderboard.\n",
        "\n",
        "    2. If it is an RL Agent, the zip file should include all the files needed to re-run the model in this submission Jupyter notebook. That will typically be the saved model and possibly the environment normalization pickle files if applicable. We will assume that all the code to load the required files and run the agent are present in the uploaded evaluation Jupiter notebook.\n",
        "\n",
        "    2. If it is a Conditional Agent the zip should contain this submission Jupyter notebook file."
      ],
      "metadata": {
        "id": "ky3zGhVp3tkf"
      }
    }
  ]
}