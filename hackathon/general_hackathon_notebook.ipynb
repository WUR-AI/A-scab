{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "458e825b680bf709",
   "metadata": {},
   "source": [
    "# ðŸ Welcome to the **Smart Droplets Hackathon**!\n",
    "\n",
    "![image](assets/sd_logo.png)\n",
    "\n",
    "The Smart Droplets project is an EU-funded project, focusing on achieving reduced pesticide and fertilizer use with techniques of Digital Twins and Reinforcement Learning. One of the pilot projects of Smart Droplets is about the reduction of the **apple scab** pest.\n",
    "In commercial apple production, **apple scab (_Venturia inaequalis_)** is the most economically important disease. Growers traditionally rely on **calendar-based fungicide programs**, which can lead to unnecessary sprays, resistance, and environmental impact.\n",
    "In this hackathon weâ€™ll flip that paradigm: **you will train a reinforcement-learning (RL) agent, or an intelligent conditional agent, to decide _when_ (and _how much_) to spray, balancing disease risk with sustainability.**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© The Challenge\n",
    "\n",
    "**Goal**: **Learn an optimal and adaptive spraying policy.**\n",
    "\n",
    "**How to achieve that?**: Use **RL** to interact with the **A-scab** disease simulator. Your agent chooses daily actions (â€œspray how muchâ€ vs. â€œdonâ€™t sprayâ€) through an entire season, to reduce the risk of breakout.\n",
    "\n",
    "**Why is this important?** Smarter timing reduces chemical use, lowers costs, and lowers environmental impact while keeping orchards healthy to reduce yield loss.\n",
    "\n",
    "Success is measured by a **cumulative reward**:\n",
    "\n",
    "$R_t = - Risk_{t} - \\beta P_t$\n",
    "\n",
    "* $Risk$ is **Infection risk** â€“ cumulative infection severity at harvest\n",
    "* $\\beta$ is **trade-off coefficient** - coefficient describing economic and ecological price of pesticides\n",
    "* $P$ is **Pesticide amount** â€“ the amount of pesticide sprayed\n",
    "\n",
    "A higher score is better -- closer to zero!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ± The Environment: **Ascabgym**\n",
    "\n",
    "Ascabgym is a stochastic, weather-driven model of apple scab dynamics, based on the A-scab model.\n",
    "At each daily time step, an agent can observe the following features:\n",
    "\n",
    "* _Weather features_: Temperature, relative humidity, rainfall, and leaf wetness\n",
    "    * also 1 day and 2 day forecasts of the above weather features\n",
    "* _Pest features_: ascopore maturity & development\n",
    "* _Tree features_: leaf area index and phenology\n",
    "\n",
    "â€¦and updates disease progress based on your action. We expose a **Gymnasium-style API** so you can plug in any RL library (Stable-Baselines3, RLlib, CleanRL, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‚ Data & Starter Kit\n",
    "\n",
    "* **Historical weather files** for training the model, files from several locations in Europe (CSV).\n",
    "* **Default orchard parameters** which are already embedded in the model\n",
    "* Performance of baseline agents to beat!\n",
    "\n",
    "% Clone the repo or fork the Kaggle notebook, pip-install extra libraries as needed, and start experimenting.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ† Evaluation & Leaderboards\n",
    "\n",
    "1. **Local validation** â€“ run episodes on the provided weather years (fast iteration).\n",
    "2. **Public leaderboard** â€“ on submission, Kaggle simulates **four hidden seasons** from a particular location and reveals your average score.\n",
    "3. **Private leaderboard** (final ranking) â€“ after the deadline we evaluate on **five additional unseen seasons** to prevent overfitting.\n",
    "\n",
    "> **Tip:** Aim for generalisation, not leaderboard-hacking!\n",
    "\n",
    "---\n",
    "\n",
    "## âœ‹ Rules of the Game\n",
    "\n",
    "| Topic | Rule                                                                                               |\n",
    "|-------|----------------------------------------------------------------------------------------------------|\n",
    "| **External data** | In general, the use of external data is not allowed.                                               |\n",
    "| **Compute** | Submissions must run < **3 hours on Kaggleâ€™s 2Ã—V100** quota.                                       |\n",
    "| **Team size** | Individual!                                                                                        |\n",
    "| **Fair play** | No model-sharing between teams until after the hackathon closes. Respect Kaggleâ€™s Code of Conduct. |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ Resources\n",
    "\n",
    "* **A-scab docs** (PDF + API reference)\n",
    "* **RL documentation** (Stable Baselines 3 documentation)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551061fc3142e30a",
   "metadata": {},
   "source": [
    "## ðŸš€ First things first\n",
    "\n",
    "Install all the necessary packages. Feel free to install the RL framework you feel most comfortable working with. By default, you can use Stable Baselines 3. Make sure to install the hackathon version of the A-scab repository."
   ]
  },
  {
   "cell_type": "code",
   "id": "f3cafe6104fa35be",
   "metadata": {},
   "source": [
    "from mpmath import hyper\n",
    "#clone the repository and install AscabGym\n",
    "!rm -rf A-scab/\n",
    "!git clone -b hackathon https://github.com/WUR-AI/A-scab.git\n",
    "#change directory\n",
    "%cd A-scab\n",
    "#install poetry if needed.\n",
    "!pip install -qqq poetry\n",
    "\n",
    "#install; this step may take 5 minutes\n",
    "!poetry config virtualenvs.in-project true\n",
    "!poetry install --quiet --all-extras\n",
    "\n",
    "# These are things you don't need to know about for now :)\n",
    "import os, sys\n",
    "\n",
    "VENV_PATH = \"/content/A-scab/.venv/lib/python3.11/site-packages\"\n",
    "LOCAL_VENV_PATH = '/content/venv' # local notebook\n",
    "os.symlink(VENV_PATH, LOCAL_VENV_PATH) # connect to directory in drive\n",
    "sys.path.insert(0, LOCAL_VENV_PATH)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe2605ff0e5bda89",
   "metadata": {},
   "source": [
    "# install further packages using poetry, to keep package dependency intact\n",
    "# in general the syntax is `poetry add PACKAGE_NAME`, then install with `poetry install`.\n",
    "# below is an example how to install (rllib)[https://docs.ray.io/en/latest/rllib/rllib-algorithms.html\n",
    "\n",
    "# note: you don't have to install it now for this notebook!\n",
    "!poetry add ray[rllib]\n",
    "!poetry install"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f6037755e785f884",
   "metadata": {},
   "source": [
    " Initialize the gymnasium environment by importing the necessary methods. You must do `import ascab` in this hackathon!\n",
    "\n",
    "This may take 1 minute or so :)"
   ]
  },
  {
   "cell_type": "code",
   "id": "c26950f41d5b4447",
   "metadata": {},
   "source": [
    "import os\n",
    "import ascab\n",
    "import gymnasium as gym"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6d53b6ece951abbf",
   "metadata": {},
   "source": [
    "For this hackathon, we have provided pre-registered environments that you will only need the ID to call. For example, the code below will construct the AscabGym environment!"
   ]
  },
  {
   "cell_type": "code",
   "id": "3196dc192da94e0c",
   "metadata": {},
   "source": [
    "ascab_train = gym.make('AscabTrainEnv-Discrete')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c44ad5787aeb10c6",
   "metadata": {},
   "source": [
    "The code below will be your validation gym environment, i.e., the place where you test your trained agents! (Note the different environment id)"
   ]
  },
  {
   "cell_type": "code",
   "id": "f140f59007503ac1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T15:12:58.185417Z",
     "start_time": "2025-05-21T15:12:58.145059Z"
    }
   },
   "source": [
    "ascab_val = gym.make('AscabValEnv-Discrete')"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T15:16:39.184629Z",
     "start_time": "2025-05-21T15:16:39.180462Z"
    }
   },
   "cell_type": "code",
   "source": "ascab_train.weather_keys",
   "id": "1fd1c05a419215d0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baja001.WUR\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ascab-r05znchY-py3.11\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001B[33mWARN: env.weather_keys to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.weather_keys` for environment variables or `env.get_wrapper_attr('weather_keys')` that will search the reminding wrappers.\u001B[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['41.5_3.0924_2016-02-01_2024-11-02']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "b3de232b133441b0",
   "metadata": {},
   "source": [
    "Additionally, if you don't plan on training an RL agent, please use the environment below:"
   ]
  },
  {
   "cell_type": "code",
   "id": "be6e95d21e3de79c",
   "metadata": {},
   "source": [
    "ascab_val_nonrl = gym.make('AscabValEnv-Continuous-NonRL')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8f7517dad01f05dc",
   "metadata": {},
   "source": [
    "# Let's get to know the Ascab environment! (_Gym speed dating?_)\n",
    "\n",
    "We're gonna introduce to you the _action space_, the _observation space_ and the _goal_ of the A-scab gym environment!\n",
    "\n",
    "### Action space, A.K.A what your agent can do!\n",
    "\n",
    "with the code below, you can check what the agent can do in each timestep!"
   ]
  },
  {
   "cell_type": "code",
   "id": "1f026fcfbe433b82",
   "metadata": {},
   "source": [
    "print(ascab_train.action_space)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6fa68ee07caf18eb",
   "metadata": {},
   "source": [
    "Notice that it is a discrete action space of 6? This corresponds to the following amounts: $ A = \\{0.2\\,i\\,|\\,i=0,1,2,\\dots,6\\}$.\n",
    "\n",
    "You can also use a continuous action spaces, which will have actions of [0, 1]! That will allow the agent to have a more fine-grained decision when spraying.\n",
    "\n",
    "_hint: initialize the environment with `gym.make(AscabTrainEnv-Continuous')`_\n",
    "\n",
    "Let's try checking what the agent can do: let's sample an action:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "9ac50d4b6fb044ae",
   "metadata": {},
   "source": [
    "# Try running this cell a few times: you should see different things the agent can do in the AscabGym!\n",
    "print(ascab_train.action_space.sample())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8df1343d29bc4b18",
   "metadata": {},
   "source": [
    "See different numbers pop up? What do they mean? Remember the formula above! 1 means the agent sprays 0.2 pesticide at that day, and so on. 0 just means the agent did not spray at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5666b020728b7ab",
   "metadata": {},
   "source": [
    "### Observation space, A.K.A what your agent can see!\n",
    "\n",
    "The in general, the agent can see these following features:\n",
    "\n",
    "#### *Fungus (n = 2)*\n",
    "\n",
    "| Feature           | Description                                                      | Units |\n",
    "|-------------------|------------------------------------------------------------------|-------|\n",
    "| InfectionWindow   | An indicator of whether the simulation goes in the risk period   | -     |\n",
    "| SRA_discharge     | Portion of ascopores becoming airborne during a discharge event  | -     |\n",
    "\n",
    "#### *RL Agent (n = 4)*\n",
    "\n",
    "| Feature           | Description                                                      | Units |\n",
    "|-------------------|------------------------------------------------------------------|-------|\n",
    "| AppliedPesticide  | Total amount of applied pesticide                                | -     |\n",
    "| ActionHistory     | Total number of spraying events in the growing season            | -     |\n",
    "| RemainingSprays   | Indicator of number of spraying events left                      | -     |\n",
    "| Î² (beta)          | Trade-off coefficient[^a]                                        | -     |\n",
    "\n",
    "#### *Host (n = 1)*\n",
    "\n",
    "| Feature         | Description                            | Units |\n",
    "|------------------|----------------------------------------|-------|\n",
    "| LAI              | Leaf Area Index                        | -     |\n",
    "\n",
    "#### *Weather (n = 15)*\n",
    "\n",
    "| Feature         | Description                                                                 | Units   |\n",
    "|------------------|-----------------------------------------------------------------------------|---------|\n",
    "| LWD              | Leaf wetness duration                                                       | hours   |\n",
    "| Precip           | Amount of precipitation                                                     | mm      |\n",
    "| Temp             | Average temperature                                                         | Â°C      |\n",
    "| HasRainEvent     | Presence of significant rainfall (â‰¥ 0.2 mm/h)                               | -       |\n",
    "| HighHumDur       | Hours of high humidity (â‰¥ 85%)                                              | hours   |\n",
    "\n",
    "---\n",
    "\n",
    "[^a]: Used in the reward function.\n",
    "[^b]: Weather variables are computed for the current day and the two-day forecast (5 variables Ã— 3 days), aggregated over 24 hours.\n",
    "\n",
    "You can also check this through code:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ba7aff0227896cd4",
   "metadata": {},
   "source": [
    "print(ascab_train.observation_space)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b31ca4090898b80a",
   "metadata": {},
   "source": [
    "A bit confusing maybe, but each dictionary key represents the features, or what your RL agent can \"see\". It directly maps from the table above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d6b641d1705a5c",
   "metadata": {},
   "source": [
    "### Cool! But what's the goal here? _How do I $\\mathcal{W}\\mathcal{I}\\mathcal{N}$_? ðŸ§\n",
    "\n",
    "Oh OK, eager are we? To win, you have to minimize risk by spraying pesticide! But also, don't spray too much. The environment will not like that.\n",
    "\n",
    "You're going to train (or create) a decision-making agent that optimally sprays pesticide to minimize risk! Check again the reward formula from above:\n",
    "\n",
    "$R_t = - Risk_{t} - \\beta P_t$\n",
    "\n",
    "* $Risk$ is **Infection risk** â€“ cumulative infection severity at harvest\n",
    "* $\\beta$ is **trade-off coefficient** - coefficient describing economic and ecological price of pesticides\n",
    "* $P$ is **Pesticide amount** â€“ the amount of pesticide sprayed\n",
    "\n",
    "Ultimately, you want to minimize cumulative risk per year!\n",
    "\n",
    "Let's get a hands-on to see how reward works:"
   ]
  },
  {
   "cell_type": "code",
   "id": "43a5846c80dbfa85",
   "metadata": {},
   "source": [
    "# reset the environment\n",
    "_, _ = ascab_train.reset()\n",
    "\n",
    "# Now let's try doing an action of spraying half-amount\n",
    "action = 3\n",
    "\n",
    "_, reward, _, _, info = ascab_train.step(action)\n",
    "\n",
    "print(f\"I sprayed {action} and got a reward of {reward:.03} :(\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "84ad154433ad51c9",
   "metadata": {},
   "source": [
    "Did you see a negative reward signal from spraying? Too bad... But we actually need to spray to minimize risk of infections. But not too much! Your task is then to find this balance. Feel free to change the value of action and see how it changes the reward you get.\n",
    "\n",
    "Try running the next code block to see how risk affects reward:"
   ]
  },
  {
   "cell_type": "code",
   "id": "64b87899d13cc7a3",
   "metadata": {},
   "source": [
    "# reset the environment\n",
    "_, _ = ascab_train.reset()\n",
    "\n",
    "# let's just not do anything for now and see what happens in the environment\n",
    "action = 0\n",
    "\n",
    "_, reward, _, _, info = ascab_train.step(action)\n",
    "# let's loop until we get some risk going on:\n",
    "while info['Risk'][-1] < 0.01:\n",
    "    _, reward, _, _, info = ascab_train.step(action)\n",
    "\n",
    "print(f\"I did not spray and now I see some apple scab on the leaves!\\n\"\n",
    "      f\"I now have a risk of {info['Risk'][-1]:.03} and got a reward of {reward:.03} :(\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7e0e482f42b4e6e2",
   "metadata": {},
   "source": [
    "Uh oh, with high risk there is a chance of yield loss, and nobody wants that! Now that you understand how the reward function works, let's build an agent that can learn when to optimally spray!~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f739fb9204a521b0",
   "metadata": {},
   "source": [
    "# Example of training your own agent\n",
    "\n",
    "Here we show you how to train your own agent! You can either\n",
    "1. Train an RL agent with your own framework.\n",
    "or\n",
    "2. Create your own intelligence conditional agent!\n",
    "\n",
    "Scroll below for further instructions.\n",
    "\n",
    "### Training with Stable Baselines 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d100b96c80d590f",
   "metadata": {},
   "source": [
    "Below we provide an example of training your RL-based pesticide expert with the DQN algorithm, provided by the Stable Baselines 3 algorithm. You can use it by using our defined `RL_Agent` class! Otherwise, you are free to create your own class by subclassing `Base_Agent`. Confused? Feel free to ask us!"
   ]
  },
  {
   "cell_type": "code",
   "id": "c58f4a5c7657e7d9",
   "metadata": {},
   "source": [
    "from ascab.train import RLAgent\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "os.makedirs(os.path.join(os.getcwd(), 'log'), exist_ok=True)\n",
    "\n",
    "log_dir = os.path.join(os.getcwd(), 'log')\n",
    "\n",
    "# Let's try using these hyperparameters\n",
    "hyperparameters = {\"learning_rate\": 0.0001, 'gamma': 1}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1ac668527c014cd4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Start tensorboard for inspecting training progress.\n",
    "# .... keep on hitting the \"refresh\" icon (the circle-with-arrow)\n",
    "\n",
    "%load_ext tensorboard\n",
    "# or %reload_ext tensorboard if you loaded it already\n"
   ],
   "id": "5d2c6889747c9062",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " # change this if you want ot point to a specific folder!\n",
    "%tensorboard --logdir ."
   ],
   "id": "1901f96a92140127",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# now train!\n",
    "rl_agent = RLAgent(\n",
    "    ascab_train=ascab_train,  # train in the training environment\n",
    "    ascab_test=ascab_val,\n",
    "    observation_filter=list(ascab_train.observation_space.keys()),\n",
    "    render=False,\n",
    "    path_model=os.path.join(os.getcwd(), 'rl_agent'),\n",
    "    path_log=os.path.join(os.getcwd(), 'log'),\n",
    "    rl_algorithm=DQN,  # feed in the call function of the Stable Baselines 3 model\n",
    "    seed=107,  # use random seed if you like to\n",
    "    n_steps=50_000,  #train it for 50k steps. NOTE: This is nowhere near enough for an agent to learn\n",
    "    hyperparameters = hyperparameters,\n",
    ")"
   ],
   "id": "fd57187e3a16264c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "During training, you can check out its running performance with tensorboard! Scroll up a bit and check out its performance. Keep hitting that refresh button!",
   "id": "cee962d5c5f37e31"
  },
  {
   "cell_type": "markdown",
   "id": "d0faeacd9a60596d",
   "metadata": {},
   "source": [
    "### \"Do I _have_ to use Stable Baselines 3?\"\n",
    "If you don't want to use the Stable Baselines 3 framework, you can of course start training using any RL framework you prefer, starting by using the `ascab_train` gym environment defined above.\n",
    "There's a bunch available! Some popular ones are:\n",
    "\n",
    "- [CleanRL](https://docs.cleanrl.dev/rl-algorithms/overview/)\n",
    "- [RLlib](https://docs.ray.io/en/latest/rllib/rllib-algorithms.html)\n",
    "- [Google Dopamine](https://github.com/google/dopamine)\n",
    "- [Meta's PeaRL](https://pearlagent.github.io)\n",
    "- [Tianshou](https://tianshou.org/en/stable/)\n",
    "- or go hardcore with [PyTorchRL](https://docs.pytorch.org/rl/stable/index.html) ðŸ˜Ž\n",
    "\n",
    "Quite some choices available huh? But don't worry, this is just a matter of taste. Each algorithm has their own pros and cons. If you want to go simple, [StableBaselines3](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html) is more than enough as a starting point!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a462cba864d79ce5",
   "metadata": {},
   "source": [
    "### \"What if I don't want to use RL?\" No problem, we got you!\n",
    "It's nevertheless possible to create your own non-RL agent; a conditional agent!\n",
    "One example of a conditional agent could be a spraying schedule based on weather forecasts. This strategy is typically employed by farmers. Here's an example of how to do it:\n",
    "\n",
    "### Example of creating a conditional agent"
   ]
  },
  {
   "cell_type": "code",
   "id": "1c4be5e87aa44125",
   "metadata": {},
   "source": [
    "from datetime import datetime\n",
    "from ascab.train import BaseAgent\n",
    "from ascab.env.env import AScabEnv\n",
    "\n",
    "# First define the class, not forgetting to SubClass `BaseAgent`\n",
    "# In general, you want to change the get_action method to apply your conditional spraying strategy\n",
    "class HowMyLocalFarmerSprays(BaseAgent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ascab: AScabEnv = None,\n",
    "        render: bool = True,\n",
    "    ):\n",
    "        super().__init__(ascab=ascab, render=render)\n",
    "\n",
    "    def get_action(self, observation: dict = None) -> float:\n",
    "        # The code below means: \"If it is forecasted that it will rain in two days, I will spray today.\n",
    "        if self.ascab.get_wrapper_attr(\"info\")[\"Forecast_day2_HasRain\"] and self.ascab.get_wrapper_attr(\"info\")[\"Forecast_day2_HasRain\"][-1]:\n",
    "            return 1.0  # the agent sprays this much if is forecasted to rain in two days\n",
    "        return 0.0\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2103ce780c3ba443",
   "metadata": {},
   "source": [
    "Then, you can try and run your expert strategy in the validation environment!"
   ]
  },
  {
   "cell_type": "code",
   "id": "eecb4f1d2b57b667",
   "metadata": {},
   "source": [
    "farmer_strategy = HowMyLocalFarmerSprays(ascab_val_nonrl, render=False)\n",
    "\n",
    "# Use the class method below to run your agent!\n",
    "farmer_strategy_results = farmer_strategy.run()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "62049c085012d60e",
   "metadata": {},
   "source": [
    "Also, you can evaluate your trained DQN RL agent the same way:"
   ]
  },
  {
   "cell_type": "code",
   "id": "6815391363580107",
   "metadata": {},
   "source": [
    "rl_results = rl_agent.run()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f0b18575a6a5e30f",
   "metadata": {},
   "source": [
    "### Want to check out how your agent did?\n",
    "\n",
    "Use the code below to plot results after using the `.run()` method!"
   ]
  },
  {
   "cell_type": "code",
   "id": "710489b108fb12",
   "metadata": {},
   "source": [
    "from ascab.utils.plot import plot_results\n",
    "\n",
    "\n",
    "# First, make a dictionary of your agents\n",
    "dict_to_plot = {\"RL DQN\":rl_results,\n",
    "                \"MyFarmer\":farmer_strategy_results,}\n",
    "\n",
    "plot_results(dict_to_plot,\n",
    "             save_path=os.getcwd(),\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d03db76ca3f98aee",
   "metadata": {},
   "source": [
    "This graph is quite dense, but it shows important information! It shows 6 features for the whole season. The top 3 are precipitation in mm, fungus development (this is the risk period!), and discharge events of the fungus.\n",
    "\n",
    "On the right you can see a zoomed in version of the bottom three during the risk period. The features are pesticide levels on the tree, the risk index of the season and the pesticide spraying actions.\n",
    "\n",
    "The label below shows the total reward each agent achieved. Zero is the highest reward.\n",
    "\n",
    "##### So, how did your agent(s) do? Not satisfied? Try another strategy or RL agent!\n",
    "\n",
    "## Want to train in your own machine? Or train in a super-computer? Piece of cake!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1568937422a71ef",
   "metadata": {},
   "source": [
    "Just install everything there, keeping in mind to use the `hackathon` branch from the A-scab repository, and download the training and testing data from the kaggle competition. Make sure to put both `train.csv` and `val.csv` are under `..\\A-scab\\dataset\\`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d310966f745acc2",
   "metadata": {},
   "source": "# Important things below! ðŸ”‘ðŸ”‘ðŸ”‘"
  },
  {
   "cell_type": "markdown",
   "id": "df2c1f4bd580413f",
   "metadata": {},
   "source": [
    "Last but not least, while we would like for you to enjoy the hackathon process, there are a few hard rules we would like to enforce:\n",
    "1. No sharing agents between teams.\n",
    "2. All RL models must be trained in the given environment(s). No training with additional data or features!\n",
    "3. Conditional agents (and therefore RL agents) are not permitted to use two features:\n",
    "    - `\"Risk\"`, this is the target :)\n",
    "    - `\"Pesticide\"`, this makes the agent cheat a bit, since it will know how much pesticide is left in the canopy.\n",
    "\n",
    "We will manually check every model during testing and submission.\n",
    "\n",
    "_Any violations could result in a disqualification from the leaderboard._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a57cdb962862704",
   "metadata": {},
   "source": [
    "# Instructions for submitting your winning Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06034d867248497",
   "metadata": {},
   "source": [
    "The testing will be done in an unseen location! Here are the instructions to submit your agent:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25dd0c1d30dd5fe",
   "metadata": {},
   "source": [
    "### For RL agents:"
   ]
  },
  {
   "cell_type": "code",
   "id": "1f5c3e088711194b",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7157004515bfb411",
   "metadata": {},
   "source": [
    "### For Conditional agents:"
   ]
  },
  {
   "cell_type": "code",
   "id": "ed0b143cd6f83b4",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
