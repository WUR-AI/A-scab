{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ðŸ Welcome to the **Smart Droplets Hackathon**!\n",
    "\n",
    "![image](assets/sd_logo.png)\n",
    "\n",
    "The Smart Droplets project is an EU-funded project, focusing on achieving reduced pesticide and fertilizer use with techniques of Digital Twins and Reinforcement Learning. One of the pilot projects of Smart Droplets is about the reduction of the **apple scab** pest.\n",
    "In commercial apple production, **apple scab (_Venturia inaequalis_)** is the most economically important disease. Growers traditionally rely on **calendar-based fungicide programs**, which can lead to unnecessary sprays, resistance, and environmental impact.\n",
    "In this hackathon weâ€™ll flip that paradigm: **you will train a reinforcement-learning (RL) agent, or an intelligent conditional agent, to decide _when_ (and _how much_) to spray, balancing disease risk with sustainability.**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© The Challenge\n",
    "\n",
    "**Goal**: **Learn an optimal and adaptive spraying policy.**\n",
    "\n",
    "**How to achieve that?**: Use **RL** to interact with the **A-scab** disease simulator. Your agent chooses daily actions (â€œspray how muchâ€ vs. â€œdonâ€™t sprayâ€) through an entire season, to reduce the risk of breakout.\n",
    "\n",
    "**Why is this important?** Smarter timing reduces chemical use, lowers costs, and lowers environmental impact while keeping orchards healthy to reduce yield loss.\n",
    "\n",
    "Success is measured by a **cumulative reward**:\n",
    "\n",
    "$R_t = - Risk_{t} - \\beta P_t$\n",
    "\n",
    "* $Risk$ is **Infection risk** â€“ cumulative infection severity at harvest\n",
    "* $\\beta$ is **trade-off coefficient** - coefficient describing economic and ecological price of pesticides\n",
    "* $P$ is **Pesticide amount** â€“ the amount of pesticide sprayed\n",
    "\n",
    "A higher score is better -- closer to zero!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ± The Environment: **Ascabgym**\n",
    "\n",
    "Ascabgym is a stochastic, weather-driven model of apple scab dynamics, based on the A-scab model.\n",
    "At each daily time step, an agent can observe the following features:\n",
    "\n",
    "* _Weather features_: Temperature, relative humidity, rainfall, and leaf wetness\n",
    "    * also 1 day and 2 day forecasts of the above weather features\n",
    "* _Pest features_: ascopore maturity & development\n",
    "* _Tree features_: leaf area index and phenology\n",
    "\n",
    "â€¦and updates disease progress based on your action. We expose a **Gymnasium-style API** so you can plug in any RL library (Stable-Baselines3, RLlib, CleanRL, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‚ Data & Starter Kit\n",
    "\n",
    "* **Historical weather files** for training the model, files from several locations in Europe (CSV).\n",
    "* **Default orchard parameters** which are already embedded in the model\n",
    "* Performance of baseline agents to beat!\n",
    "\n",
    "% Clone the repo or fork the Kaggle notebook, pip-install extra libraries as needed, and start experimenting.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ† Evaluation & Leaderboards\n",
    "\n",
    "1. **Local validation** â€“ run episodes on the provided weather years (fast iteration).\n",
    "2. **Public leaderboard** â€“ on submission, Kaggle simulates **four hidden seasons** from a particular location and reveals your average score.\n",
    "3. **Private leaderboard** (final ranking) â€“ after the deadline we evaluate on **five additional unseen seasons** to prevent overfitting.\n",
    "\n",
    "> **Tip:** Aim for generalisation, not leaderboard-hacking!\n",
    "\n",
    "---\n",
    "\n",
    "## âœ‹ Rules of the Game\n",
    "\n",
    "| Topic | Rule                                                                                               |\n",
    "|-------|----------------------------------------------------------------------------------------------------|\n",
    "| **External data** | In general, the use of external data is not allowed.                                               |\n",
    "| **Compute** | Submissions must run < **3 hours on Kaggleâ€™s 2Ã—V100** quota.                                       |\n",
    "| **Team size** | Individual!                                                                                        |\n",
    "| **Fair play** | No model-sharing between teams until after the hackathon closes. Respect Kaggleâ€™s Code of Conduct. |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ Resources\n",
    "\n",
    "* **A-scab docs** (PDF + API reference)\n",
    "* **RL documentation** (Stable Baselines 3 documentation)\n",
    "\n",
    "---\n"
   ],
   "id": "458e825b680bf709"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸš€ First things first\n",
    "\n",
    "Install all the necessary packages. Feel free to install the RL framework you feel most comfortable working with. By default, you can use Stable Baselines 3. Make sure to install the hackathon version of the A-scab repository."
   ],
   "id": "551061fc3142e30a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# install Ascabgym, note the @hackathon at the end of the repo link.\n",
    "!pip install ascab@git+https://github.com/BigDataWUR/A-scab.git@hackathon\n",
    "\n",
    "#clone the repository\n",
    "!git clone -b hackathon https://github.com/WUR-AI/A-scab.git\n",
    "#change directory\n",
    "%cd A-scab\n",
    "#install poetry if needed.\n",
    "!pip install -qqq poetry\n",
    "#install; this step may take 5 minutes\n",
    "!poetry install --quiet"
   ],
   "id": "f3cafe6104fa35be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# install further packages using poetry, to keep package dependency intact\n",
    "# in general the syntax is `poetry add PACKAGE_NAME`, then install with `poetry install`.\n",
    "# below is an example how to install stable baselines 3\n",
    "\n",
    "!poetry add stable-baselines3\n",
    "!poetry install"
   ],
   "id": "fe2605ff0e5bda89"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Initialize the gymnasium environment by importing the necessary methods:",
   "id": "f6037755e785f884"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c1448c78d9e8058e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from ascab.env.env import MultipleWeatherASCabEnv, get_weather_library_from_csv, ActionConstrainer"
   ],
   "id": "c26950f41d5b4447",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The code below will construct the gym environment!",
   "id": "6d53b6ece951abbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ascab_train = MultipleWeatherASCabEnv(\n",
    "    weather_data_library=get_weather_library_from_csv(os.path.join('content', 'dataset', 'train.csv')),\n",
    "    discrete_actions=True,  # Feel free to change this if using a method or RL agent that uses continuous actions.\n",
    ")\n"
   ],
   "id": "3196dc192da94e0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The code below will be your validation gym environment, i.e., the place where you test your trained agents!",
   "id": "c44ad5787aeb10c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ascab_val = MultipleWeatherASCabEnv(\n",
    "    weather_data_library=get_weather_library_from_csv(os.path.join('content', 'dataset', 'train.csv')),\n",
    "    discrete_actions=True,  # Feel free to change this if using a method or RL agent that uses continuous actions.\n",
    ")"
   ],
   "id": "f140f59007503ac1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Example of training your own agent\n",
    "\n",
    "### Training with Stable Baselines 3"
   ],
   "id": "f739fb9204a521b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Below we provide an example of training your RL-based pesticide expert with the DQN algorithm, provided by the Stable Baselines 3 algorithm.",
   "id": "1d100b96c80d590f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from ascab.train import RLAgent\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "os.makedirs(os.path.join(os.getcwd(), 'rl_model'), exist_ok=True)\n",
    "os.makedirs(os.path.join(os.getcwd(), 'log'), exist_ok=True)\n",
    "\n",
    "rl_agent = RLAgent(\n",
    "    ascab_train=ascab_train,\n",
    "    ascab_test=ascab_val,\n",
    "    observation_filter=list(ascab_train.observation_space.keys()),\n",
    "    render=False,\n",
    "    path_model=os.path.join(os.getcwd(), 'rl_model'),\n",
    "    path_log=os.path.join(os.getcwd(), 'log'),\n",
    "    rl_algorithm=DQN,  # feed in the constructor of the Stable Baselines 3 model\n",
    "    seed=107,  # use random seed if you like to\n",
    ")"
   ],
   "id": "c58f4a5c7657e7d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If you don't want to use the Stable Baselines 3 framework, you can of course start training using any RL framework you prefer, starting by using the `ascab_train` gym environment defined above.\n",
    "\n",
    "### Don't want to use RL? No problem, we got you!\n",
    "It's nevertheless possible to create your own non-RL agent; a conditional agent!\n",
    "One conditional agent could be a spraying schedule based on calendar date. This strategy is typically employed by farmers. Here's an example of how to do it:\n",
    "\n",
    "### Example of creating a conditional agent"
   ],
   "id": "d0faeacd9a60596d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datetime import datetime\n",
    "from ascab.train import BaseAgent\n",
    "from ascab.env.env import AScabEnv\n",
    "\n",
    "# First define the class, not forgetting to SubClass `BaseAgent`\n",
    "# In general, you want to change the get_action method to apply your conditional spraying strategy\n",
    "class HowMyLocalFarmerSprays(BaseAgent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ascab: AScabEnv = None,\n",
    "        render: bool = True,\n",
    "        dates: list[datetime.date] = None\n",
    "    ):\n",
    "        super().__init__(ascab=ascab, render=render)\n",
    "        if dates is None:\n",
    "            year = self.ascab.get_wrapper_attr(\"date\").year\n",
    "            dates = [datetime.date(year, 3, 20), datetime.date(year, 4, 1), datetime.date(year, 4, 8)]  # dates you want to spray\n",
    "        self.dates = dates\n",
    "\n",
    "    def get_action(self, observation: dict = None) -> float:\n",
    "        if self.ascab.get_wrapper_attr(\"info\")[\"Date\"] and self.ascab.get_wrapper_attr(\"info\")[\"Date\"][-1] in self.dates:\n",
    "            return 1.0  # you spray this much if when it is the date arrives\n",
    "        return 0.0\n"
   ],
   "id": "1c4be5e87aa44125"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then, you can try and run your expert strategy in the validation environment!",
   "id": "2103ce780c3ba443"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "farmer_strategy = HowMyLocalFarmerSprays(ascab_val)\n",
    "farmer_strategy.run()"
   ],
   "id": "eecb4f1d2b57b667"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "How did it do? Continue experimenting for the best spraying strategy!\n",
    "\n",
    "Also, you can evaluate a trained RL agent the same way:"
   ],
   "id": "62049c085012d60e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "rl_agent.run()",
   "id": "6815391363580107"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Example of evaluation",
   "id": "f0b18575a6a5e30f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Example of creating MyAgent",
   "id": "30dd4131893a6655"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1a45a13af4278721"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5966975ffe0966d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Instructions for submitting your winning Agent",
   "id": "6a57cdb962862704"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Obs space is fixed; allowed features\n",
    "2. Training file is fixed\n",
    "3. We provide Val file\n",
    "4. Testing for leaderboard will be done in unseen location"
   ],
   "id": "f06034d867248497"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ed0b143cd6f83b4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
