{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "458e825b680bf709",
   "metadata": {},
   "source": [
    "# 🍏 Welcome to the **Smart Droplets Hackathon**!\n",
    "\n",
    "![image](assets/sd_logo.png)\n",
    "\n",
    "The Smart Droplets project is an EU-funded project, focusing on achieving reduced pesticide and fertilizer use with techniques of Digital Twins and Reinforcement Learning. One of the pilot projects of Smart Droplets is about the reduction of the **apple scab** pest.\n",
    "In commercial apple production, **apple scab (_Venturia inaequalis_)** is the most economically important disease. Growers traditionally rely on heuristics such as **calendar-based fungicide programs**, which can lead to unnecessary sprays, resistance, and environmental impact.\n",
    "In this hackathon we’ll flip that paradigm: **you will train a reinforcement-learning (RL) agent, or an intelligent conditional agent, to decide _when_ (and _how much_) to spray, balancing disease risk with sustainability.**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/WUR-AI/A-scab/blob/hackathon/hackathon/general_hackathon_notebook.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 The Challenge\n",
    "\n",
    "**Goal**: **Learn an optimal and adaptive spraying policy.**\n",
    "\n",
    "**How to achieve that?**: Use **RL** to interact with the **A-scab** disease simulator. Your agent chooses daily actions (“spray how much” vs. “don’t spray”) through an entire season, to reduce the risk of breakout.\n",
    "\n",
    "**Why is this important?** Smarter timing reduces chemical use, lowers costs, and lowers environmental impact while keeping orchards healthy to reduce yield loss.\n",
    "\n",
    "Success is measured by a **cumulative reward**:\n",
    "\n",
    "$R_t = - Risk_{t} - \\beta P_t$\n",
    "\n",
    "* $Risk$ is **Infection risk** – cumulative infection severity at harvest\n",
    "* $\\beta$ is **trade-off coefficient** - coefficient describing economic and ecological price of pesticides\n",
    "* $P$ is **Pesticide amount** – the amount of pesticide sprayed\n",
    "\n",
    "A higher score is better -- closer to zero!\n",
    "\n",
    "---\n",
    "\n",
    "## 🌱 The Environment: **Ascabgym**\n",
    "\n",
    "Ascabgym is a stochastic, weather-driven simulation of apple scab dynamics, adapted from the A-scab model.\n",
    "At each daily time step, an agent selects a pesticide dosage based on observations related to fungal state, weather conditions, and host susceptibility.\n",
    "We provide a **Gymnasium-style API**, enabling easy integration with any reinforcement learning library (e.g., Stable-Baselines3, RLlib, CleanRL).\n",
    "\n",
    "---\n",
    "\n",
    "## 🏆 Evaluation & Leaderboards\n",
    "\n",
    "1. **Local validation** – run episodes on the provided weather years (fast iteration).\n",
    "2. **Public leaderboard** – on submission, Kaggle simulates **eight hidden seasons** from a particular location and reveals your average score.\n",
    "\n",
    "> **Tip:** Aim for generalisation, not leaderboard-hacking!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551061fc3142e30a",
   "metadata": {},
   "source": [
    "## 🚀 First things first\n",
    "\n",
    "Install all the necessary packages. Feel free to install the RL framework you feel most comfortable working with. By default, you can use Stable Baselines 3. Make sure to install the hackathon version of the A-scab repository."
   ]
  },
  {
   "cell_type": "code",
   "id": "f3cafe6104fa35be",
   "metadata": {},
   "source": [
    "from mpmath import hyper\n",
    "import os\n",
    "import sys\n",
    "import subprocess # Added for subprocess\n",
    "\n",
    "# Clone the repository\n",
    "!rm -rf A-scab/\n",
    "!git clone -b hackathon https://github.com/WUR-AI/A-scab.git\n",
    "\n",
    "# Change directory\n",
    "%cd A-scab\n",
    "\n",
    "# Get the absolute path of the project root *after* changing directory\n",
    "PROJECT_ROOT = os.getcwd()\n",
    "\n",
    "# Install poetry if needed.\n",
    "!pip install -qqq poetry\n",
    "\n",
    "# Configure poetry to create virtual environments in the project directory\n",
    "!poetry config virtualenvs.in-project true\n",
    "\n",
    "# Install project dependencies\n",
    "# This step may take some time (e.g., 5 minutes)\n",
    "!poetry install --quiet --all-extras\n",
    "\n",
    "# These are things you don't need to know about for now :)\n",
    "# 1. Add the project root to sys.path\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "    print(f\"Added project root {PROJECT_ROOT} to sys.path\")\n",
    "\n",
    "# 2. Get the path to the site-packages directory of the poetry virtual environment\n",
    "try:\n",
    "    venv_path_output = subprocess.check_output(['poetry', 'env', 'info', '--path']).decode('utf-8').strip()\n",
    "    python_version_major_minor = f\"python{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "    SITE_PACKAGES_PATH = os.path.join(venv_path_output, 'lib', python_version_major_minor, 'site-packages')\n",
    "except Exception as e:\n",
    "    print(f\"Could not determine poetry venv site-packages path using 'poetry env info': {e}\")\n",
    "    print(\"Attempting a common fallback path structure...\")\n",
    "    SITE_PACKAGES_PATH = os.path.abspath(os.path.join('.venv', 'lib', f'python{sys.version_info.major}.{sys.version_info.minor}', 'site-packages'))\n",
    "\n",
    "# 3. Add the site-packages directory to sys.path\n",
    "if os.path.exists(SITE_PACKAGES_PATH) and SITE_PACKAGES_PATH not in sys.path:\n",
    "    sys.path.insert(0, SITE_PACKAGES_PATH)\n",
    "    print(f\"Added {SITE_PACKAGES_PATH} to sys.path\")\n",
    "else:\n",
    "    print(f\"Warning: Could not find site-packages at {SITE_PACKAGES_PATH} or it's already in sys.path.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe2605ff0e5bda89",
   "metadata": {},
   "source": [
    "# install further packages using poetry, to keep package dependency intact\n",
    "# in general the syntax is `poetry add PACKAGE_NAME`, then install with `poetry install`.\n",
    "# below is an example how to install (rllib)[https://docs.ray.io/en/latest/rllib/rllib-algorithms.html\n",
    "\n",
    "# note: you don't have to install it now for this notebook!\n",
    "!poetry add ray[rllib]\n",
    "!poetry install"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f6037755e785f884",
   "metadata": {},
   "source": [
    " Initialize the gymnasium environment by importing the necessary methods. You must do `import ascab` in this hackathon!\n",
    "\n",
    "This may take 1 minute or so :)"
   ]
  },
  {
   "cell_type": "code",
   "id": "c26950f41d5b4447",
   "metadata": {},
   "source": [
    "import os\n",
    "import ascab\n",
    "import gymnasium as gym"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6d53b6ece951abbf",
   "metadata": {},
   "source": [
    "For this hackathon, we have provided pre-registered environments that you will only need the ID to call. For example, the code below will construct the AscabGym environment!"
   ]
  },
  {
   "cell_type": "code",
   "id": "3196dc192da94e0c",
   "metadata": {},
   "source": [
    "ascab_train = gym.make('AscabTrainEnv-Discrete')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c44ad5787aeb10c6",
   "metadata": {},
   "source": [
    "The code below will be your validation gym environment, i.e., the place where you test your trained agents! (Note the different environment id)"
   ]
  },
  {
   "cell_type": "code",
   "id": "f140f59007503ac1",
   "metadata": {},
   "source": [
    "ascab_val = gym.make('AscabValEnv-Discrete')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b3de232b133441b0",
   "metadata": {},
   "source": [
    "Additionally, if you don't plan on training an RL agent, please use the environment below:"
   ]
  },
  {
   "cell_type": "code",
   "id": "be6e95d21e3de79c",
   "metadata": {},
   "source": [
    "ascab_val_nonrl = gym.make('AscabValEnv-Continuous-NonRL')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8f7517dad01f05dc",
   "metadata": {},
   "source": [
    "# Let's get to know the Ascab environment! (_Gym speed dating?_)\n",
    "\n",
    "We're gonna introduce to you the _action space_, the _observation space_ and the _goal_ of the A-scab gym environment!\n",
    "\n",
    "### Action space, A.K.A what your agent can do!\n",
    "\n",
    "with the code below, you can check what the agent can do in each timestep!"
   ]
  },
  {
   "cell_type": "code",
   "id": "1f026fcfbe433b82",
   "metadata": {},
   "source": [
    "print(ascab_train.action_space)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6fa68ee07caf18eb",
   "metadata": {},
   "source": [
    "Notice that it is a discrete action space of 6? This corresponds to the following amounts: $ A = \\{0.2\\,i\\,|\\,i=0,1,2,\\dots,6\\}$.\n",
    "\n",
    "You can also use a continuous action spaces, which will have actions of [0, 1]! That will allow the agent to have a more fine-grained decision when spraying.\n",
    "\n",
    "_hint: initialize the environment with `gym.make(AscabTrainEnv-Continuous')`_\n",
    "\n",
    "Let's try checking what the agent can do: let's sample an action:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "9ac50d4b6fb044ae",
   "metadata": {},
   "source": [
    "# Try running this cell a few times: you should see different things the agent can do in the AscabGym!\n",
    "print(ascab_train.action_space.sample())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8df1343d29bc4b18",
   "metadata": {},
   "source": [
    "See different numbers pop up? What do they mean? Remember the formula above! 1 means the agent sprays 0.2 pesticide at that day, and so on. 0 just means the agent did not spray at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5666b020728b7ab",
   "metadata": {},
   "source": [
    "### Observation space, A.K.A what your agent can see!\n",
    "\n",
    "The in general, the agent can see these following features:\n",
    "\n",
    "#### *Fungus (n = 2)*\n",
    "\n",
    "| Feature           | Description                                                      | Units |\n",
    "|-------------------|------------------------------------------------------------------|-------|\n",
    "| InfectionWindow   | An indicator of whether the simulation goes in the risk period   | -     |\n",
    "| SRA_discharge     | Portion of ascopores becoming airborne during a discharge event  | -     |\n",
    "\n",
    "#### *RL Agent (n = 4)*\n",
    "\n",
    "| Feature          | Description                                           | Units |\n",
    "|------------------|-------------------------------------------------------|-------|\n",
    "| AppliedPesticide | Total amount of applied pesticide                     | -     |\n",
    "| ActionHistory    | Total number of spraying events in the growing season | -     |\n",
    "| SinceLastAction  | Number of days since last spraying event              | -     |\n",
    "| β (beta)         | Trade-off coefficient[^a]                             | -     |\n",
    "\n",
    "#### *Host (n = 1)*\n",
    "\n",
    "| Feature         | Description                            | Units |\n",
    "|------------------|----------------------------------------|-------|\n",
    "| LAI              | Leaf Area Index                        | -     |\n",
    "\n",
    "#### *Weather (n = 15)*\n",
    "\n",
    "| Feature         | Description                                                                 | Units   |\n",
    "|------------------|-----------------------------------------------------------------------------|---------|\n",
    "| LWD              | Leaf wetness duration                                                       | hours   |\n",
    "| Precip           | Amount of precipitation                                                     | mm      |\n",
    "| Temp             | Average temperature                                                         | °C      |\n",
    "| HasRainEvent     | Presence of significant rainfall (≥ 0.2 mm/h)                               | -       |\n",
    "| HighHumDur       | Hours of high humidity (≥ 85%)                                              | hours   |\n",
    "\n",
    "---\n",
    "\n",
    "[^a]: Used in the reward function.\n",
    "[^b]: Weather variables are computed for the current day and the two-day forecast (5 variables × 3 days), aggregated over 24 hours.\n",
    "\n",
    "You can also check this through code:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ba7aff0227896cd4",
   "metadata": {},
   "source": [
    "print(ascab_train.observation_space)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b31ca4090898b80a",
   "metadata": {},
   "source": [
    "A bit confusing maybe, but each dictionary key represents the features, or what your RL agent can \"see\". It directly maps from the table above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d6b641d1705a5c",
   "metadata": {},
   "source": [
    "### Cool! But what's the goal here? _How do I $\\mathcal{W}\\mathcal{I}\\mathcal{N}$_? 🧐\n",
    "\n",
    "Oh OK, eager are we? To win, you have to minimize risk by spraying pesticide! But also, don't spray too much. The environment will not like that.\n",
    "\n",
    "You're going to train (or create) a decision-making agent that optimally sprays pesticide to minimize risk! Check again the reward formula from above:\n",
    "\n",
    "$R_t = - Risk_{t} - \\beta P_t$\n",
    "\n",
    "* $Risk$ is **Infection risk** – cumulative infection severity at harvest\n",
    "* $\\beta$ is **trade-off coefficient** - coefficient describing economic and ecological price of pesticides\n",
    "* $P$ is **Pesticide amount** – the amount of pesticide sprayed\n",
    "\n",
    "Ultimately, you want to minimize cumulative risk per year!\n",
    "\n",
    "Let's get a hands-on to see how reward works:"
   ]
  },
  {
   "cell_type": "code",
   "id": "43a5846c80dbfa85",
   "metadata": {},
   "source": [
    "# reset the environment\n",
    "_, _ = ascab_train.reset()\n",
    "\n",
    "# Now let's try doing an action of spraying half-amount\n",
    "action = 3\n",
    "\n",
    "_, reward, _, _, info = ascab_train.step(action)\n",
    "\n",
    "print(f\"I sprayed {action} and got a reward of {reward:.03} :(\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "84ad154433ad51c9",
   "metadata": {},
   "source": [
    "Did you see a negative reward signal from spraying? Too bad... But we actually need to spray to minimize risk of infections. But not too much! Your task is then to find this balance. Feel free to change the value of action and see how it changes the reward you get.\n",
    "\n",
    "Try running the next code block to see how risk affects reward:"
   ]
  },
  {
   "cell_type": "code",
   "id": "64b87899d13cc7a3",
   "metadata": {},
   "source": [
    "# reset the environment\n",
    "_, _ = ascab_train.reset()\n",
    "\n",
    "# let's just not do anything for now and see what happens in the environment\n",
    "action = 0\n",
    "\n",
    "_, reward, _, _, info = ascab_train.step(action)\n",
    "# let's loop until we get some risk going on:\n",
    "while info['Risk'][-1] < 0.01:\n",
    "    _, reward, _, _, info = ascab_train.step(action)\n",
    "\n",
    "print(f\"I did not spray and now I see some apple scab on the leaves!\\n\"\n",
    "      f\"I now have a risk of {info['Risk'][-1]:.03} and got a reward of {reward:.03} :(\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7e0e482f42b4e6e2",
   "metadata": {},
   "source": [
    "Uh oh, with high risk there is a chance of yield loss, and nobody wants that! Now that you understand how the reward function works, let's build an agent that can learn when to optimally spray!~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f739fb9204a521b0",
   "metadata": {},
   "source": [
    "# Example of training your own agent\n",
    "\n",
    "Here we show you how to train your own agent! You can either\n",
    "1. Train an RL agent with your own framework.\n",
    "or\n",
    "2. Create your own intelligence conditional agent!\n",
    "\n",
    "Scroll below for further instructions.\n",
    "\n",
    "### Training with Stable Baselines 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d100b96c80d590f",
   "metadata": {},
   "source": [
    "Below we provide an example of training your RL-based pesticide expert with the DQN algorithm, provided by the Stable Baselines 3 algorithm. You can use it by using our defined `RL_Agent` class! Otherwise, you are free to create your own class by subclassing `Base_Agent`. Confused? Feel free to ask us!"
   ]
  },
  {
   "cell_type": "code",
   "id": "c58f4a5c7657e7d9",
   "metadata": {},
   "source": [
    "from ascab.train import RLAgent\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "os.makedirs(os.path.join(os.getcwd(), 'log'), exist_ok=True)\n",
    "\n",
    "log_dir = os.path.join(os.getcwd(), 'log')\n",
    "\n",
    "# Let's try using these hyperparameters\n",
    "hyperparameters = {\"learning_rate\": 0.0001, 'gamma': 1}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here, we first instantiate tensorboard, so you can track the training progress of your RL agents!",
   "id": "1ac668527c014cd4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Start tensorboard.\n",
    "# .... keep on hitting the \"refresh\" icon (the circle-with-arrow) during training\n",
    "\n",
    "%load_ext tensorboard\n",
    "# or %reload_ext tensorboard if you loaded it already\n"
   ],
   "id": "5d2c6889747c9062",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " # change this dot if you want ot point to a specific folder!\n",
    "%tensorboard --logdir ."
   ],
   "id": "1901f96a92140127",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# now train!\n",
    "rl_agent = RLAgent(\n",
    "    ascab_train=ascab_train,  # train in the training environment\n",
    "    ascab_test=ascab_val,\n",
    "    observation_filter=list(ascab_train.observation_space.keys()),\n",
    "    render=False,\n",
    "    path_model=os.path.join(os.getcwd(), 'rl_agent'),\n",
    "    path_log=os.path.join(os.getcwd(), 'log'),\n",
    "    rl_algorithm=DQN,  # feed in the call function of the Stable Baselines 3 model\n",
    "    seed=107,  # use random seed if you like to\n",
    "    n_steps=50_000,  #train it for 50k steps. NOTE: This is nowhere near enough for an agent to learn\n",
    "    hyperparameters = hyperparameters,\n",
    ")"
   ],
   "id": "fd57187e3a16264c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "During training, you can check out its running performance with tensorboard! Scroll up a bit and check out its performance. Keep hitting that refresh button!",
   "id": "cee962d5c5f37e31"
  },
  {
   "cell_type": "markdown",
   "id": "d0faeacd9a60596d",
   "metadata": {},
   "source": [
    "### \"Do I _have_ to use Stable Baselines 3?\"\n",
    "If you don't want to use the Stable Baselines 3 framework, you can of course start training using any RL framework you prefer, starting by using the `ascab_train` gym environment defined above.\n",
    "There's a bunch available! Some popular ones are:\n",
    "\n",
    "- [CleanRL](https://docs.cleanrl.dev/rl-algorithms/overview/)\n",
    "- [RLlib](https://docs.ray.io/en/latest/rllib/rllib-algorithms.html)\n",
    "- [Google Dopamine](https://github.com/google/dopamine)\n",
    "- [Meta's PeaRL](https://pearlagent.github.io)\n",
    "- [Tianshou](https://tianshou.org/en/stable/)\n",
    "- or go hardcore with [PyTorchRL](https://docs.pytorch.org/rl/stable/index.html) 😎\n",
    "\n",
    "Quite some choices available huh? But don't worry, this is just a matter of taste. Each algorithm has their own pros and cons. If you want to go simple, [StableBaselines3](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html) is more than enough as a starting point!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a462cba864d79ce5",
   "metadata": {},
   "source": [
    "### \"What if I don't want to use RL?\" No problem, we got you!\n",
    "It's nevertheless possible to create your own non-RL agent; a conditional agent!\n",
    "One example of a conditional agent could be a spraying schedule based on weather forecasts. This strategy is typically employed by farmers. Here's an example of how to do it:\n",
    "\n",
    "### Example of creating a conditional agent"
   ]
  },
  {
   "cell_type": "code",
   "id": "1c4be5e87aa44125",
   "metadata": {},
   "source": [
    "from datetime import datetime\n",
    "from ascab.train import BaseAgent\n",
    "from ascab.env.env import AScabEnv\n",
    "\n",
    "# First define the class, not forgetting to SubClass `BaseAgent`\n",
    "# In general, you want to change the `get_action` method to apply your conditional spraying strategy\n",
    "class HowMyLocalFarmerSprays(BaseAgent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ascab: AScabEnv = None,\n",
    "        render: bool = True,\n",
    "    ):\n",
    "        super().__init__(ascab=ascab, render=render)\n",
    "\n",
    "    def get_action(self, observation: dict = None) -> float:\n",
    "        # The code below means: \"If it is forecasted that it will rain in two days, I will spray today\".\n",
    "        if self.ascab.get_wrapper_attr(\"info\")[\"Forecast_day2_HasRain\"] and self.ascab.get_wrapper_attr(\"info\")[\"Forecast_day2_HasRain\"][-1]:\n",
    "            return 1.0  # the agent sprays this much if is forecasted to rain in two days\n",
    "        return 0.0\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2103ce780c3ba443",
   "metadata": {},
   "source": [
    "Then, you can try and run your expert strategy in the validation environment!"
   ]
  },
  {
   "cell_type": "code",
   "id": "eecb4f1d2b57b667",
   "metadata": {},
   "source": [
    "farmer_strategy = HowMyLocalFarmerSprays(ascab_val_nonrl, render=False)\n",
    "\n",
    "# Use the class method below to run your agent!\n",
    "farmer_strategy_results = farmer_strategy.run()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "62049c085012d60e",
   "metadata": {},
   "source": [
    "Also, you can evaluate your trained DQN RL agent the same way:"
   ]
  },
  {
   "cell_type": "code",
   "id": "6815391363580107",
   "metadata": {},
   "source": [
    "rl_results = rl_agent.run()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f0b18575a6a5e30f",
   "metadata": {},
   "source": [
    "### Want to check out how your agent did?\n",
    "\n",
    "Use the code below to plot results after using the `.run()` method!"
   ]
  },
  {
   "cell_type": "code",
   "id": "710489b108fb12",
   "metadata": {},
   "source": [
    "from ascab.utils.plot import plot_results\n",
    "\n",
    "\n",
    "# First, make a dictionary of your agents\n",
    "dict_to_plot = {\"RL DQN\":rl_results,\n",
    "                \"MyFarmer\":farmer_strategy_results,}\n",
    "\n",
    "plot_results(dict_to_plot,\n",
    "             save_path=os.getcwd(),\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d03db76ca3f98aee",
   "metadata": {},
   "source": [
    "These graphs are quite dense, but it shows important information! It shows 6 features for the whole season. The top 3 are precipitation in millimetres, fungus development (this is the risk period!), and discharge events of the fungus.\n",
    "\n",
    "On the right you can see a zoomed in version of the bottom three during the risk period. The features are pesticide levels on the tree, the risk index of the season and the pesticide spraying actions.\n",
    "\n",
    "The label below shows the total reward each agent achieved. Zero is the highest reward.\n",
    "\n",
    "##### So, how did your agent(s) do? Not satisfied? Try another strategy or RL agent!\n",
    "\n",
    "## Want to train in your own machine? Or train in a super-computer? Piece of cake!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1568937422a71ef",
   "metadata": {},
   "source": "Just install everything there, as we show in the first code block of this notebook. Keep in mind to use the `hackathon` branch from the A-scab repository, and download the training and testing data from the kaggle competition. Make sure to put both `train.csv` and `val.csv` are under `..\\A-scab\\dataset\\`."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Is there a model to beat? How do I know I'm doing good?",
   "id": "894744a7a50c8dcc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Good question! Below we show the performance of an RL agent that Hilmy trained. The table below shows the validation environment.\n",
    "\n",
    "#### Below we show the performance of an RL agent that Hilmy trained, evaluated on the `AscabValEnv-Discrete` env for every year\n",
    "\n",
    "\n",
    "| No  | 2017 | 2019 | 2021 | 2023 |\n",
    "| --- | -------- | -------- | -------- | -------- |\n",
    "| 1   | -0.07      | -0.09       | -0.04        | -0.06       |\n",
    "\n",
    "It does quite well huh? When trained properly, RL can do great things!"
   ],
   "id": "9e4d2e4b381dc6a0"
  },
  {
   "cell_type": "markdown",
   "id": "6d310966f745acc2",
   "metadata": {},
   "source": "# 🔑🔑🔑 Important things below! 🔑🔑🔑"
  },
  {
   "cell_type": "markdown",
   "id": "df2c1f4bd580413f",
   "metadata": {},
   "source": [
    "Last but not least, while we would like for you to enjoy the hackathon process, there are a few hard rules we would like to enforce:\n",
    "1. No sharing agents between teams.\n",
    "2. All RL models must be trained in the given environment(s). No training with additional data or features!\n",
    "3. Conditional agents (and therefore RL agents) are not permitted to use two features:\n",
    "    - `\"Risk\"`, this is the target :)\n",
    "    - `\"Pesticide\"`, this makes the agent(s) cheat a bit, since it will know how much pesticide is left in the canopy.\n",
    "\n",
    "If you're wondering if Hilmy's RL agent adhered to rules 2 and 3 to get the above results: _yes, he did adhere to them_ 😎\n",
    "\n",
    "We will manually check every model during testing and submission.\n",
    "\n",
    "_Any violations could result in a disqualification from the leaderboard._\n",
    "\n",
    "Specific questions? Feel free to ask us directly or send us a quick email!\n",
    "michiel.kallenberg@wur.nl, hilmy.baja@wur.nl, zehao.lu@wur.nl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a57cdb962862704",
   "metadata": {},
   "source": [
    "# Instructions for submitting your winning Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06034d867248497",
   "metadata": {},
   "source": [
    "The testing will be done in an unseen location! Here are the instructions to submit your agent:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25dd0c1d30dd5fe",
   "metadata": {},
   "source": [
    "### For RL agents:"
   ]
  },
  {
   "cell_type": "code",
   "id": "1f5c3e088711194b",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7157004515bfb411",
   "metadata": {},
   "source": [
    "### For Conditional agents:"
   ]
  },
  {
   "cell_type": "code",
   "id": "ed0b143cd6f83b4",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
